---
layout: page
title: internal world
permalink: /thoughts/internalworld
---

people have said that it seems that i have a rich internal world

years ago, i would have agreed, but as of now i don't enjoy my internal world nearly as much. what does that mean, anyways? to have a rich internal world, isn't it kind of just saying that you [don't](/thoughts/meanreversion) have a rich external world? does it just mean i'm autistic? and when i put my internal world in text like this, it's kind of external now, right?

there's nothing really going on in my head most of the time any more. meow. i still have a little bit of imagination, but i really miss being able to draw out complex worlds, like the city in a pencil, or the apartment complex in the bathtub tiles, just for fun, or just for a fleeting erotic idea. i can still build out these worlds. it just takes longer, maybe because of all of the knowledge of the real world i've accumulated as an adult. if i was to retreat from the world, like i did as a child, would that bring back my imagination?

one time [strawberry tree](/friends/strawberrytree) and i were talking about the world as a simulation. if the simulation is running on a computer somewhere in a physical world with physical constraints and costs, like ours, then they would want to optimize it. they would cut out unnecessary computation. the little quantum fluctuations in the core of a rock don't really matter unless someone happens to be looking very very closely at that moment. so they'd use some coarser model that simulates the rock as a whole.

but what does matter? what do the simulators actually care about, what are they looking for in this world? do they simulate every gnat's flight? every distant star's life cycle? every person's thoughts? *any* person's thoughts? and do they even simulate the past at all? or maybe the only thing that exists is the computation for the present moment. and all of the world's memories are nothing more than AI filling in plausible backstories, just like a large language model fills in plausible next words. "i'm probably not fully simulated," i say to strawberry tree. "yeah no shit," he replies. but maybe if i had more interesting thoughts or more meaningful connections or something, i would be important to whatever the simulators are looking for. and their algorithm would decide to expend the compute to simulate me. and i would become physically real. it seems unlikely that the line would be drawn between human minds and other human minds, but not impossible.




